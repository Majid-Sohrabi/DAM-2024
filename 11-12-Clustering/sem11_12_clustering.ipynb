{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Seminar 11-12"
      ],
      "metadata": {
        "id": "jRmlFP9bK-yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Supervised Learning** - we have target (correct answer), model is trained to predict it. Regression, classification are supervised methods. Examples: predict prices of houses, predict if the passenger will survive Titanic accident, etc\n",
        "*   **Unsupervised Learning** - no right answer, just input data. Goal is to find some dependencies, groups, etc. Example: information about students of HSE - what can be said about them? What groups do we have?\n",
        "\n"
      ],
      "metadata": {
        "id": "EXsRWeatO_WM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "V_b5dLePP8fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLustering is a unsupervised method. Main idea is to separate data into groups (cluster), each cluster should contain similar samples and different clusters should contain different samples."
      ],
      "metadata": {
        "id": "sOKM1o9iP-Xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with the example. How many groups of data do we seem to have? Which point is in which group?"
      ],
      "metadata": {
        "id": "nCWA1sgeRLoe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCAAplNjKx-1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#Generate some data\n",
        "np.random.seed(123)\n",
        "X1 = np.random.randn(100,2)\n",
        "X2 = np.random.randn(100,2) - np.array([10,1])\n",
        "X3 = np.random.randn(100,2) - np.array([1,10])\n",
        "X = np.vstack((X1,X2,X3))\n",
        "\n",
        "plt.scatter(X[:, 0], X[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KMeans"
      ],
      "metadata": {
        "id": "tmmpVr9sRXN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Idea"
      ],
      "metadata": {
        "id": "FI8C1Sewu9Xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans is a clustering algorithm. It has a hyperparameter $k$ (number of clusters)\n",
        "\n",
        "The algorithm:\n",
        "\n",
        "\n",
        "\n",
        "1.   Initialize $k$ cluster centers randomly.\n",
        "2.   Assign points to corresponding clusters (with minimum distance to their center).\n",
        "3. Recalculate cluster centers using mean for all points belonging to the cluster.\n",
        "4. Steps 2-3 are repeated until cluster centers stop changing (significantly)."
      ],
      "metadata": {
        "id": "lc_BTVqNRaGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3 #Let's find 3 groups/clusters"
      ],
      "metadata": {
        "id": "b-XJwsL3SAr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement it:\n",
        "\n",
        "*Step 1* initialize cluster centers."
      ],
      "metadata": {
        "id": "n5O_fXTZR2Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "centers = np.random.uniform(low=X.min(),high=X.max(),size=(k, X.shape[1]))\n",
        "\n",
        "plt.scatter(X[:, 0], X[:,1], label='data')\n",
        "plt.scatter(centers[:, 0], centers[:,1], label='random centers', color='black')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "ZmpymN4OR8aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 2* Assign points to the nearest cluster"
      ],
      "metadata": {
        "id": "sUWDm5G9Snen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "#for every observation compute the distance to the center\n",
        "dists = cdist(X, centers)\n",
        "\n",
        "dists.shape"
      ],
      "metadata": {
        "id": "phsFHawVSt_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dists.argmin(axis=1)\n",
        "labels.shape"
      ],
      "metadata": {
        "id": "6R-TZ7nfTFbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster in sorted(np.unique(labels)):\n",
        "  plt.scatter(X[labels == cluster][:, 0], X[labels == cluster][:,1], label='cluster ' + str(cluster))\n",
        "plt.scatter(centers[:, 0], centers[:,1], label='centers', color='black', marker='x')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "CDDpb4_wTMaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 3* Recalculate centers"
      ],
      "metadata": {
        "id": "B1ac1YMwTj3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster in sorted(np.unique(labels)):\n",
        "  centers[cluster] = np.mean(X[labels == cluster], axis=0)\n",
        "\n",
        "for cluster in sorted(np.unique(labels)):\n",
        "  plt.scatter(X[labels == cluster][:, 0], X[labels == cluster][:,1], label='cluster ' + str(cluster))\n",
        "plt.scatter(centers[:, 0], centers[:,1], label='recalculated centers', color='black', marker='x')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "2zDXjFwQTjle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 4* - Repeat steps 2-3"
      ],
      "metadata": {
        "id": "p9tXkP95T3tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for every observation compute the distance to the center\n",
        "dists = cdist(X, centers)\n",
        "#set the new labels\n",
        "labels = dists.argmin(axis=1)\n",
        "\n",
        "#plot new clusters\n",
        "for cluster in sorted(np.unique(labels)):\n",
        "  plt.scatter(X[labels == cluster][:, 0], X[labels == cluster][:,1], label='cluster ' + str(cluster))\n",
        "plt.scatter(centers[:, 0], centers[:,1], label='centers', color='black', marker='x')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "sdxgIIgOT8em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#recalculate centers\n",
        "for cluster in sorted(np.unique(labels)):\n",
        "  centers[cluster] = np.mean(X[labels == cluster], axis=0)\n",
        "\n",
        "#plot old clusters and new centers\n",
        "for cluster in sorted(np.unique(labels)):\n",
        "  plt.scatter(X[labels == cluster][:, 0], X[labels == cluster][:,1], label='cluster ' + str(cluster))\n",
        "plt.scatter(centers[:, 0], centers[:,1], label='recalculated centers', color='black', marker='x')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Hm1RflxKUWQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recalculated centers do not differ much from the ones on the previous steps. Seems like we are almost finished"
      ],
      "metadata": {
        "id": "HiP2EiLFUjpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for every observation compute the distance to the center\n",
        "dists = cdist(X, centers)\n",
        "#set the new labels\n",
        "labels = dists.argmin(axis=1)\n",
        "\n",
        "#recalculate centers\n",
        "for cluster in sorted(np.unique(labels)):\n",
        "  centers[cluster] = np.mean(X[labels == cluster], axis=0)\n",
        "\n",
        "#plot old clusters and new centers\n",
        "for cluster in sorted(np.unique(labels)):\n",
        "  plt.scatter(X[labels == cluster][:, 0], X[labels == cluster][:,1], label='cluster ' + str(cluster))\n",
        "plt.scatter(centers[:, 0], centers[:,1], label='recalculated centers', color='black', marker='x')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "cqC4CaMNU0Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new clustering does not differ at all, all the points have the same labels (points did not change colors) and centers are in the same place. We should stop the algorithm."
      ],
      "metadata": {
        "id": "AdoNRH55U304"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans divided data into 3 clusters and the division is nice."
      ],
      "metadata": {
        "id": "T-WdY-6mVpE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's check KMeans from sklearn."
      ],
      "metadata": {
        "id": "8Tlu86Q0Vza0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k_means = KMeans(n_clusters = 3)\n",
        "#Fit using only X (there are no target/y)\n",
        "k_means = k_means.fit(X)\n",
        "clusters = k_means.predict(X)\n",
        "\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c = clusters)\n",
        "plt.title('KMeans clustering')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lNouQDjxV-n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we did not set the number of clusters ($k$) correctly"
      ],
      "metadata": {
        "id": "ZfKpvZFPWKe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize= (15,8))\n",
        "for n_c in range(2,8):\n",
        "    k_means = KMeans(n_clusters = n_c)\n",
        "    k_means = k_means.fit(X)\n",
        "    clusters = k_means.predict(X)\n",
        "    plt.subplot(2,3,n_c - 1)\n",
        "    plt.scatter(X[:,0], X[:,1], c = clusters)\n",
        "    plt.title('n_clusters = {}'.format(n_c))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sMKbaX3XoShh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can we see:\n",
        "\n",
        "\n",
        "*   KMeans tries to assign points to every cluster (it cannot reduce/increase number of clusters)\n",
        "*   Clusters are convex (looks like circles, ovals, spheres in multidimensional space)\n",
        "*   Clusters have similar sizes (number of points)\n",
        "\n"
      ],
      "metadata": {
        "id": "WG6Cn0RWojlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The advantage of the algorith is the speed (one iteration - just recalculating distances)"
      ],
      "metadata": {
        "id": "JLbaG4mlpU8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The disdvantages:\n",
        "\n",
        "\n",
        "*   Random initialization of cluster centers (may get different clusters)\n",
        "*   Assumptions about cluster shapes, sizes\n",
        "\n"
      ],
      "metadata": {
        "id": "J2KBd21UpoUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Visualization of KMeans](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)"
      ],
      "metadata": {
        "id": "O1qMSYrEp7Tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering metrics"
      ],
      "metadata": {
        "id": "xCEJs0vUvCuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to search for hyperparameters? We do not have target to assess the quality.\n",
        "\n",
        "Possible approach is to use the internal clustering validation measures (just try to measure, how good are the groups)\n",
        "\n",
        "We need to measure:\n",
        "\n",
        "* cohesion - the closer samples in one cluster, the better\n",
        "* separation - the further different clusters are from each other, the better\n",
        "\n"
      ],
      "metadata": {
        "id": "ATfDKh54u4cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example,  Silhouette Coefficient:\n",
        "\n",
        "$$Sil(X, C) = \\frac{1}{|C|}\\sum_{c_k \\in C}\\frac{1}{|c_k|}\\sum_{x_i \\in c_k}\\frac{separation(x_i, c_k) - cohension(x_i, c_k)}{max\\{separation(x_i, c_k) , cohension(x_i, c_k)\\}}$$\n",
        "\n",
        "where\n",
        "\n",
        "$separation(x_i, c_k) = min_{c_l \\in C ∖ \\{c_k\\}} \\{\\frac{1}{|c_l|}\\sum_{x_j \\in c_l}||x_i - x_j||\\}$ - mean distance to the object of the closest cluster;\n",
        "\n",
        "\n",
        "$cohension(x_i, c_k) = \\frac{1}{|c_k|-1}\\sum_{x_j \\in c_k}||x_i - x_j||$ - mean distance to the objects of the same cluster\n",
        "\n",
        "\n",
        "$-1 \\le Sil(X, C) \\le 1$\n",
        "\n",
        "The closer it to 1, the better. If it is near 0, the clusters are overlapping (same disnace from the point to the closest and its own cluster). Negative score means that samples have wrong labels (objects are closer to other cluster than to its own cluster)\n"
      ],
      "metadata": {
        "id": "750hibqEwI7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "best_k, best_score = None, -1\n",
        "for k in range(2,15):\n",
        "    k_means = KMeans(n_clusters = k)\n",
        "    k_means = k_means.fit(X)\n",
        "    clusters = k_means.predict(X)\n",
        "    score = silhouette_score(X=X,\n",
        "                             labels=clusters)\n",
        "    print(k, score)\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_k = k\n",
        "print('Best score {}, k = {}'.format(best_score, best_k))"
      ],
      "metadata": {
        "id": "3tjMoIE6xCzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fails"
      ],
      "metadata": {
        "id": "JdrEF8E6vApc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the data, where KMeans fail.\n",
        "\n",
        "The data looks like two moons. Most likely, we need to separate data int 2 clusters: upper and lower moon"
      ],
      "metadata": {
        "id": "VKJz_54TqHYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons, make_circles\n",
        "X_moon, y_moon = make_moons(n_samples=500, noise=0.1) #y_moon contain info about what moon the point belong to in reality\n",
        "\n",
        "plt.scatter(X_moon[:,0], X_moon[:,1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3_slwXfyqK3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_means = KMeans(n_clusters = 2)\n",
        "k_means = k_means.fit(X_moon)\n",
        "clusters = k_means.predict(X_moon)\n",
        "\n",
        "plt.scatter(X_moon[:,0], X_moon[:,1], c=clusters)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sGClm851qTIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans separated convex clusters  Let's look where the centers are:"
      ],
      "metadata": {
        "id": "U5FhUGU6qYNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_moon[:,0], X_moon[:,1], c=clusters)\n",
        "\n",
        "for cl in np.unique(clusters):\n",
        "  center_ = X_moon[clusters == cl].mean(axis=0)\n",
        "  plt.scatter([center_[0]], [center_[1]], label='Center cluster ' + str(cl), marker='x')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OUgXWXD8pncX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did KMeans fail? Because clusters are non-convex and the part of one cluster is closer to the centriod of another. KMeans does not allow it."
      ],
      "metadata": {
        "id": "-znRnMhnsNve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reality\n",
        "plt.scatter(X_moon[:,0], X_moon[:,1], c=y_moon)\n",
        "\n",
        "for cl in np.unique(y_moon):\n",
        "  center_ = X_moon[y_moon == cl].mean(axis=0)\n",
        "  plt.scatter([center_[0]], [center_[1]], label='Center of moon ' + str(cl), marker='x')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o1wjClBxsCmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example:"
      ],
      "metadata": {
        "id": "yED0_khntFz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X_circle, y_circle = make_circles(n_samples=500, noise=0.05, factor=0.5)\n",
        "plt.scatter(X_circle[:,0], X_circle[:,1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bTEjTf8DtFgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, seems to be 2 clusters: inner and outer ring"
      ],
      "metadata": {
        "id": "PmgBShHHtt6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_means = KMeans(n_clusters = 2)\n",
        "k_means = k_means.fit(X_circle)\n",
        "clusters = k_means.predict(X_circle)\n",
        "plt.scatter(X_circle[:,0], X_circle[:,1], c=clusters)\n",
        "\n",
        "for cl in np.unique(clusters):\n",
        "  center_ = X_circle[clusters == cl].mean(axis=0)\n",
        "  plt.scatter([center_[0]], [center_[1]], label='Center cluster ' + str(cl), marker='x')\n",
        "\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ugv15XFwt0N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reality\n",
        "plt.scatter(X_circle[:,0], X_circle[:,1], c=clusters)\n",
        "\n",
        "for cl in np.unique(y_circle):\n",
        "  center_ = X_circle[y_circle == cl].mean(axis=0)\n",
        "  plt.scatter([center_[0]], [center_[1]], label='Center ring ' + str(cl), marker='x')\n",
        "\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ugpIUXxVuF7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And another one:"
      ],
      "metadata": {
        "id": "efgZbq5K2h6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X_blob, y_blob = make_blobs(centers=[[-10, 0], [10, 0]], n_samples=100)\n",
        "X_blob[0] = [200, 0]\n",
        "\n",
        "plt.scatter(X_blob[:,0], X_blob[:,1])"
      ],
      "metadata": {
        "id": "1uB43-rB21Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have 2 blobs (depicted as thin ovals) and one outlier."
      ],
      "metadata": {
        "id": "X8AZ008O4YnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_means = KMeans(n_clusters = 2)\n",
        "k_means = k_means.fit(X_blob)\n",
        "clusters = k_means.predict(X_blob)\n",
        "plt.scatter(X_blob[:,0], X_blob[:,1], c=clusters)\n",
        "\n",
        "for cl in np.unique(clusters):\n",
        "  center_ = X_blob[clusters == cl].mean(axis=0)\n",
        "  plt.scatter([center_[0]], [center_[1]], label='Center cluster ' + str(cl), marker='x')\n",
        "\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nF9ZwzFY3S6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here there is an outlier - point really far from other data. KMeans considers it to be a separate cluster (because it makes center of the cluster shift far right and there is no reason to add it to another cluster)."
      ],
      "metadata": {
        "id": "YFHDUvs53mmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN"
      ],
      "metadata": {
        "id": "Cjefavsu1yfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN - Density-based spatial clustering of applications with noise"
      ],
      "metadata": {
        "id": "7siqZS6J15Vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea: dense areas (areas where are a lot of points) are the clusters, we need to move along dense areas to find the edge of it (less dense area next to dense area). If there are points very far from the dense areas, this points are considered *noise* (points which do not belong to any cluster)"
      ],
      "metadata": {
        "id": "wUqHW9cn19xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters:\n",
        "\n",
        "\n",
        "\n",
        "*   `eps` - the redius of a sphere\n",
        "*   `min_samples` - the minimum number of point in the area (sphere of radius `eps`) to consider the area dense\n",
        "\n"
      ],
      "metadata": {
        "id": "NCq25hd84tKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea of an algorithm:\n",
        "\n",
        "\n",
        "\n",
        "1.  Look into some point, draw a sphere of radius `eps` and compute the number of points in it (including the point itself)\n",
        "2.   If there are less than `min_samples` points, we need to search for another point\n",
        "3. If there are more or equal points this point belong to a cluster, all its neighbours are in this cluster, too\n",
        "4. Investigate neighbours of the point: if they also have enogh naighbours in the sphere, we should consider their neighbours, etc; if not enough - this points are the edge of the cluster (we do not consider their neighbours)\n",
        "5. After we run out of neighbours, move to step 1\n",
        "6. If we considered all the left points and there are no more clusters - mark points as noise\n",
        "\n"
      ],
      "metadata": {
        "id": "rxXy65xU5ToD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the example: `min_samples=4`"
      ],
      "metadata": {
        "id": "I5p1KZP67WB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p><a href=\"https://commons.wikimedia.org/wiki/File:DBSCAN-Illustration.svg#/media/Файл:DBSCAN-Illustration.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/1200px-DBSCAN-Illustration.svg.png\" alt=\"DBSCAN-Illustration.svg\" width=\"450\" height=\"450\"> </a><br> <a href=\"//commons.wikimedia.org/wiki/User:Chire\" title=\"User:Chire\">Chire</a> &mdash; <span class=\"int-own-work\" lang=\"ru\"></span><a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>"
      ],
      "metadata": {
        "id": "e32mbRQX5NNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Visualization of DBSCAN](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)\n"
      ],
      "metadata": {
        "id": "pob8YNm27iZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply DBSCAN from sklearn to our data from the beginning of the seminar."
      ],
      "metadata": {
        "id": "DZ577cPe759u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib as mpl\n",
        "\n",
        "#function to get nice colors\n",
        "def get_colors(clusters):\n",
        "  n_clusters = len(np.unique(clusters[clusters != -1]))\n",
        "\n",
        "  palette = mpl.colormaps['viridis'].resampled(n_clusters)(np.linspace(0, 1, n_clusters))\n",
        "  colors = np.zeros((clusters.shape[0], 4))\n",
        "  colors[clusters != -1] = palette[clusters[clusters != -1]]\n",
        "\n",
        "  #Noise will be transparent grey\n",
        "  colors[clusters == -1] = [0.5, 0.5, 0.5, 0.3]\n",
        "\n",
        "  return colors\n",
        "\n",
        "plt.figure(figsize= (15,20))\n",
        "i = 1\n",
        "\n",
        "for e in [0.2, 1, 3, 5, 10]:\n",
        "    for samples in [2, 5, 40]:\n",
        "        dbscan = DBSCAN(eps=e, min_samples=samples)\n",
        "        clusters = dbscan.fit_predict(X)\n",
        "\n",
        "        n_clusters = len(np.unique(clusters[clusters != -1]))\n",
        "        colors = get_colors(clusters)\n",
        "\n",
        "        plt.subplot(5, 3, i)\n",
        "\n",
        "        plt.scatter(X[:,0], X[:,1], c=colors)\n",
        "        plt.title('eps = {}, min_samples = {} -> {} clusters'.format(e, samples, n_clusters))\n",
        "        i += 1\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FIVxUhvy7wme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the datasets, where KMeans failed:"
      ],
      "metadata": {
        "id": "r_a2q4ab8Doj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=0.2, min_samples=10)\n",
        "clusters = dbscan.fit_predict(X_moon)\n",
        "\n",
        "colors = get_colors(clusters)\n",
        "plt.scatter(X_moon[:,0], X_moon[:,1], c=colors)\n",
        "plt.title('eps = {}, min_samples = {} -> {} clusters'.format(dbscan.eps, dbscan.min_samples, len(np.unique(clusters[clusters != -1]))))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yThEL4868H2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=0.2, min_samples=10)\n",
        "clusters = dbscan.fit_predict(X_circle)\n",
        "colors = get_colors(clusters)\n",
        "plt.scatter(X_circle[:,0], X_circle[:,1], c=colors)\n",
        "plt.title('eps = {}, min_samples = {} -> {} clusters'.format(dbscan.eps, dbscan.min_samples, len(np.unique(clusters[clusters != -1]))))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1aaoOIEx8M9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=2, min_samples=10)\n",
        "clusters = dbscan.fit_predict(X_blob)\n",
        "colors = get_colors(clusters)\n",
        "plt.scatter(X_blob[:,0], X_blob[:,1], c=colors)\n",
        "plt.title('eps = {}, min_samples = {} -> {} clusters'.format(dbscan.eps, dbscan.min_samples, len(np.unique(clusters[clusters != -1]))))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "awjYk_0Q8S7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN can manage these tricky examples. It does not use centers of clusters, so the non-convex clusters do not bother it."
      ],
      "metadata": {
        "id": "bZhcb2Gx8dXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical clustering"
      ],
      "metadata": {
        "id": "xMXPxBc49Ll3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approaches:\n",
        "\n",
        "\n",
        "\n",
        "*   Agglomerative: This is a \"bottom-up\" approach: Beginning: each observation is a cluster. Iteration: 2 clusters are merged. End: 1 cluster.\n",
        "*  Divisive: This is a \"top-down\" approach: Beginning: 1 cluster . Iteration: 1 cluster is divided in 2. End: each observation is a cluster."
      ],
      "metadata": {
        "id": "zBePZTAzbj2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use agglomerative approach.\n",
        "\n",
        "Idea: greedy build clusters\n",
        "\n",
        "Algorithm:\n",
        "\n",
        "1.   Initially every point is a separate cluster\n",
        "2.   During iteration the closest clusters are merged into one.\n",
        "3.   Repeat step 2 until there is only 1 cluster (which contains all points)\n",
        "\n"
      ],
      "metadata": {
        "id": "1CYdC6k79Sjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to compute distance between clusters (group of points)?\n",
        "\n",
        "For example, Ward's Method:\n",
        "\n",
        "\n",
        "$$\n",
        "\\Delta = \\sum_{x_i \\in A \\cup B}{(x_i-\\bar{x})^2} - \\sum_{x_i \\in A}(x_i - \\bar{a})^2 - \\sum_{x_i \\in B}(x_i - \\bar{b})^2\n",
        "$$\n"
      ],
      "metadata": {
        "id": "lh7SQAUKJ_d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It may be useful to use dendrogram to check, how the clustering works:"
      ],
      "metadata": {
        "id": "5xCQOXqlKqCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "#Code to draw dendrogram:\n",
        "#https://scikit-learn.org/1.5/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack(\n",
        "        [model.children_, model.distances_, counts]\n",
        "    ).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "\n",
        "random_state = 170\n",
        "\n",
        "\n",
        "# setting distance_threshold=0 ensures we compute the full tree.\n",
        "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
        "\n",
        "model = model.fit(X)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "# plot the top three levels of the dendrogram\n",
        "plot_dendrogram(model, truncate_mode=\"level\", p=5)\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w6mlwwVz8pzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is going on the plot:\n",
        "\n",
        "\n",
        "\n",
        "*   x axis is an id of a point (or number of points in little cluster, if it is in brackets)\n",
        "*   y axis is a distance between clusters\n",
        "*  when 2 clusters are united П is drawn, uniting them, the place of horizontal line is the distance between the united clusters\n",
        "\n"
      ],
      "metadata": {
        "id": "xMXbQJPDK2oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to choose the clustering? (Here we gat a lot of candidates)\n",
        "\n",
        "Basic approaces:\n",
        "\n",
        "\n",
        "\n",
        "*   Number of clusters. If we want particular number of clusters, we may set `n_clusters` hyperparameter (draw a line on the dendrogram such that it crosses particular number of vertical lines)\n",
        "*   Distance. We can set `distance_threshold` if the distance for next cluster merging is more than it, we need to stop (just draw horizontal line with corresponding y)\n",
        "*  \"Jump\" in dendrogram. If we can see a drastic increase in distnce (the horizontal line soars), that means we started uniting far clusters and we need to stop\n",
        "\n"
      ],
      "metadata": {
        "id": "l2OHsQxkKpFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#choosing only 3 clusters\n",
        "model = AgglomerativeClustering(n_clusters=3)\n",
        "\n",
        "clusters = model.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c = clusters)\n",
        "plt.title('Agglomerative clustering')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2Ecy8sMPMhEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the distance threshold as 80\n",
        "#need to set n_clusters to None (bacause we can use only one approach)\n",
        "model = AgglomerativeClustering(distance_threshold=80, n_clusters=None)\n",
        "\n",
        "clusters = model.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c = clusters)\n",
        "plt.title('Agglomerative clustering')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2pCzI86PM2lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real data"
      ],
      "metadata": {
        "id": "zO7nsPjBQ1fV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply clustering to some new data"
      ],
      "metadata": {
        "id": "YFjBOhdoRh6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, let's try to work with pictures of hand-written digits (one picture - one digit)"
      ],
      "metadata": {
        "id": "02OE2KdNRq0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "X, y = digits.data, digits.target\n",
        "Im = digits.images\n",
        "\n",
        "for i in range(20):\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(Im[i], cmap='gray')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Zw9yp7UVRm4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we work with pictures? Black and write picture is actually a 2D matrix - every pixel is assosiated with 1 number (brightness)"
      ],
      "metadata": {
        "id": "KgWEPdaBR9xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Im[i]"
      ],
      "metadata": {
        "id": "zfywiTIOSLVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zeros is black, 16 is white.\n",
        "\n",
        "We can flatten our matrix:"
      ],
      "metadata": {
        "id": "jb0tCUvsSRck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Im[i].flatten()"
      ],
      "metadata": {
        "id": "KMe5jxvTScc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So a 8x8 picture is actually a vector of size 64 (the features are 'how bright is the pixel i')"
      ],
      "metadata": {
        "id": "bF7pWRWGSfNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "JqdxOjd3Se5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We actually can build a classification model and get good accuracy:"
      ],
      "metadata": {
        "id": "V59RoZQ1Te-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000, C=1).fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "J6ge_FTAS5iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But let's try to apply clustering:"
      ],
      "metadata": {
        "id": "Sw8F_xi3TlCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(n_clusters=10, max_iter=100, n_init=1)\n",
        "km.fit(X)"
      ],
      "metadata": {
        "id": "96pongi1TpKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Silhouette Coefficient:\", silhouette_score(X, km.labels_, sample_size=1000))"
      ],
      "metadata": {
        "id": "fAanpORfTuoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clustering metric does not seem to be very big, but it is greater than 0."
      ],
      "metadata": {
        "id": "Jaa9XXpIT14C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But can we compare clustering to the target (if we have it)? Can we use classification metrics?"
      ],
      "metadata": {
        "id": "4pYn_08QT9m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's try:\n",
        "accuracy_score(y, km.labels_)"
      ],
      "metadata": {
        "id": "ikowNx8yUJwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy seems to be bad. Why? Because classification wants particular labels (we cannot just swap 1 and 0 inside the prediction, we would be wrong), but for clustering the label (number of the cluster) does not matter at all.\n",
        "\n",
        "If we want to compare clustering labels to classification we need to use external clustering validation measures.\n",
        "\n",
        "\n",
        "For example, V measure - it assess if the cluters contain same class and same class contains same clusters"
      ],
      "metadata": {
        "id": "Rocyu-5OUP9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import v_measure_score\n",
        "\n",
        "print('V-measure:', v_measure_score(y, km.labels_))"
      ],
      "metadata": {
        "id": "b-UZQ1xnU0J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cXS3nuYefMLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the clustering and classification almost agree"
      ],
      "metadata": {
        "id": "uNOT57JHVHh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the clustering!\n",
        "\n",
        "But how... (we have 64 features, we cannot draw such plots)"
      ],
      "metadata": {
        "id": "xOPwQMctVTQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality reduction, visualization"
      ],
      "metadata": {
        "id": "7w_IaPnsVeQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to depict some random pairs of features"
      ],
      "metadata": {
        "id": "KNpPdSorViTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y,\n",
        "            edgecolor='none', alpha=0.7, s=40,\n",
        "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "plt.colorbar()\n",
        "plt.title('MNIST. 2 features');"
      ],
      "metadata": {
        "id": "38Uz2k6FWDaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "plt.scatter(X[:, 3], X[:, 45], c=y,\n",
        "            edgecolor='none', alpha=0.7, s=40,\n",
        "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "plt.colorbar()\n",
        "plt.title('MNIST. 2 features');"
      ],
      "metadata": {
        "id": "cfvJuG3BWRWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's strange. Why? - the values are integers (0-16), so, the scatterplots will be sparse and the brightness of 1 pixel may not be important (all numbers may have particular pixel black/white, it's combination what matters).\n"
      ],
      "metadata": {
        "id": "gDxBxmzaWVB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we need to use some method, that will reduce the dimensionality of data saving as much info as possible"
      ],
      "metadata": {
        "id": "LqkdH7ZJWssA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "P-u9UpzMXP3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is using linear algebra. It searches for some linear combination of features such that it preserves as much information as possible"
      ],
      "metadata": {
        "id": "BLyKfWBsXQ3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "model_pca = PCA(n_components=2)\n",
        "X_reduced = model_pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y,\n",
        "            edgecolor='none', alpha=0.7, s=40,\n",
        "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "plt.colorbar()\n",
        "plt.title('MNIST. PCA projection');"
      ],
      "metadata": {
        "id": "wqiNDSvhXcw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA can be used not only for visualization, but just for dimensionality reduction (get new shrinked data, use later)"
      ],
      "metadata": {
        "id": "KGspC22xYEaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pca = PCA(n_components=10)\n",
        "X_reduced = model_pca.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
        "\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000, C=1).fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "8g1P7l0PY7s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE"
      ],
      "metadata": {
        "id": "u7md2R1mY8r8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE is a method for visualizaion. It trains new vector representation in such way that similar objects have close vectors and dis-similar have different vectors with big probability. It is a non-linear method, used uaually only for visualization."
      ],
      "metadata": {
        "id": "HobfvX1iY-KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_jobs=4, perplexity=30, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y,\n",
        "            edgecolor='none', alpha=0.7, s=40,\n",
        "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "\n",
        "plt.colorbar()\n",
        "plt.title('MNIST. t-SNE projection');"
      ],
      "metadata": {
        "id": "FrB0D3hhX5Lt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}