{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Seminar 9"
      ],
      "metadata": {
        "id": "3rYbQFUa5rGx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPwOrDykwzOW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First ensemble"
      ],
      "metadata": {
        "id": "KKUidFDp5msT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data similar from the data from the lecture"
      ],
      "metadata": {
        "id": "9doMVhtxyJGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "cloud1 = np.random.multivariate_normal(mean=[  0.0, 0.0], cov=[[1.0, 0.0], [0.0, 0.5]], size=50)\n",
        "cloud2 = np.random.multivariate_normal(mean=[-15.0, 0.0], cov=[[1.0, 0.0], [0.0, 0.5]], size=50)\n",
        "cloud3 = np.random.multivariate_normal(mean=[ 15.0, 0.0], cov=[[1.0, 0.0], [0.0, 0.5]], size=50)\n",
        "\n",
        "\n",
        "data = np.concatenate([cloud1, cloud2, cloud3])\n",
        "data.shape"
      ],
      "metadata": {
        "id": "2uaYAfrRxQ4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target (class)"
      ],
      "metadata": {
        "id": "vYYy6wl3yKg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = -np.ones(len(data), dtype='int32')\n",
        "y[:50] = 1"
      ],
      "metadata": {
        "id": "Xyj0-5BPyBrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "4AtWFJpd6agh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's draw a scatterplot"
      ],
      "metadata": {
        "id": "q8Enli8tyu8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(data[:,0], data[:,1],c=y)\n",
        "\n",
        "#Setting limits for x2\n",
        "plt.ylim([-6, 6])"
      ],
      "metadata": {
        "id": "4I0kQlfwyMdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is not linearly separated - impossible to draw a straight line and separate yellow class from purple"
      ],
      "metadata": {
        "id": "DP2Xg0znyx3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to build a model, similar from the one from the lecture"
      ],
      "metadata": {
        "id": "I20yBLzc0OLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1"
      ],
      "metadata": {
        "id": "3sKMOamO53yZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to separate right cloud from the middle and left one using linear model:\n",
        "\n",
        "if $x_1w_1 + w_2x_2 + w_0 > 0$ - class 1\n",
        "\n",
        "if $x_1w_1 + w_2x_2 + w_0 < 0$ - class -1"
      ],
      "metadata": {
        "id": "jrLW_ZLr0tzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to change weights to find the correct model\n",
        "w_0 = 1.0\n",
        "w_1 = 1.0\n",
        "w_2 = 1.0"
      ],
      "metadata": {
        "id": "p8zhdodw0nWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depict the model:"
      ],
      "metadata": {
        "id": "DPV2RlSH1P0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate x1\n",
        "x_1 = np.arange(data[:,0].min(), data[:,0].max()+0.2, 0.05)\n",
        "#calculate x2\n",
        "x_2_model_1 = -(w_0 + x_1 * w_1) / w_2\n",
        "plt.plot(x_1, x_2_model_1, label='Model 1')\n",
        "\n",
        "\n",
        "plt.scatter(data[:,0], data[:,1],c=y)\n",
        "plt.ylim([-6, 6])\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "UKB5Kd0D0d2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check quality:"
      ],
      "metadata": {
        "id": "Sg0ZphSu2eTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_combination1 = w_1 * data[:,0] + w_2 * data[:,1] + w_0\n",
        "linear_combination1"
      ],
      "metadata": {
        "id": "2TYyyxxo2f6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction1 = np.sign(linear_combination1).astype('int')\n",
        "prediction1"
      ],
      "metadata": {
        "id": "LTvjD_lc23nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the quality (using accuracy)"
      ],
      "metadata": {
        "id": "0W9lQNJx3PIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y, prediction1)"
      ],
      "metadata": {
        "id": "StMtsRKb3TAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is not ideal. Let's check, if our model is better than classifierthat predicts the most popular class."
      ],
      "metadata": {
        "id": "FFMMrEhE3nYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Looking at the classes in y and the number of samples\n",
        "np.unique(y, return_counts=True)"
      ],
      "metadata": {
        "id": "84M0pWLt4I0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-1 is the modt popular class\n",
        "#predicting always -1 and computing accuracy\n",
        "accuracy_score(y, -np.ones(len(y)))"
      ],
      "metadata": {
        "id": "OprIXbeL4Qmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is the same. So, our model was not so great."
      ],
      "metadata": {
        "id": "0tUCbUAx4asL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2"
      ],
      "metadata": {
        "id": "3SVxRlaq56g3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now. let's conduct the same experiment with the left cloud (separate it from the rest)"
      ],
      "metadata": {
        "id": "VGBePZkB4sC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to change weights to find the correct model\n",
        "v_0 = 1.0\n",
        "v_1 = 1.0\n",
        "v_2 = 1.0"
      ],
      "metadata": {
        "id": "_Cy8mDzr42oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate x1\n",
        "x_1 = np.arange(data[:,0].min(), data[:,0].max()+0.2, 0.05)\n",
        "#calculate x2\n",
        "x_2_model_2 = -(v_0 + x_1 * v_1) / v_2\n",
        "plt.plot(x_1, x_2_model_2, label='Model 1')\n",
        "\n",
        "\n",
        "plt.scatter(data[:,0], data[:,1],c=y)\n",
        "plt.ylim([-6, 6])\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "2n6dPLju44d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the model and measure accuracy:"
      ],
      "metadata": {
        "id": "3AOIf2SX5WZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_combination2 = v_1 * data[:,0] + v_2 * data[:,1] + v_0\n",
        "prediction2 = np.sign(linear_combination2).astype('int')\n",
        "prediction2"
      ],
      "metadata": {
        "id": "8v3K-ATh6C1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y, prediction2)"
      ],
      "metadata": {
        "id": "jeSEvVi_5ZEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got same results"
      ],
      "metadata": {
        "id": "Yw94Pthv5e4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1 and Model 2"
      ],
      "metadata": {
        "id": "tfYoQApE57p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's understand, where the model make mistakes and how to correct them"
      ],
      "metadata": {
        "id": "xeXXlJGZ59J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sep - how the words should be separated\n",
        "print('Target', 'Model 1', 'Model 2', sep='\\t')\n",
        "\n",
        "#Choosing every fifth element\n",
        "for i in range(0, len(data), 5):\n",
        "  print(y[i], prediction1[i], prediction2[i], sep='\\t')"
      ],
      "metadata": {
        "id": "k8d1WIzd61N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude:\n",
        "\n",
        "\n",
        "\n",
        "*   If both models predict 1, we should predict 1\n",
        "*   If models contradict, we should predict -1\n",
        "*   Models do not predict -1 together\n",
        "\n",
        "How can we get the best possible prediction based on the prediction of both models? - For example, we can multiply them (1 * 1 = 1, -1 * 1 = -1)\n",
        "\n"
      ],
      "metadata": {
        "id": "tLaC12BZ7jE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can build an ensemble - we know how to merge models:"
      ],
      "metadata": {
        "id": "mLg7mZJ18R1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_model_prediction(data):\n",
        "  #Copy-paste the weights from the experiments\n",
        "  w_0 = 1.0\n",
        "  w_1 = 1.0\n",
        "  w_2 = 1.0\n",
        "\n",
        "  v_0 = 1.0\n",
        "  v_1 = 1.0\n",
        "  v_2 = 1.0\n",
        "\n",
        "  #Model 1\n",
        "  linear_combination1 = w_1 * data[:,0] + w_2 * data[:,1] + w_0\n",
        "  prediction1 = np.sign(linear_combination1).astype('int')\n",
        "\n",
        "  #Model 2\n",
        "  linear_combination2 = v_1 * data[:,0] + v_2 * data[:,1] + v_0\n",
        "  prediction2 = np.sign(linear_combination2).astype('int')\n",
        "\n",
        "  #ensemble prediction\n",
        "  prediction = prediction1 * prediction2\n",
        "\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "nyJd9XQf8d8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it"
      ],
      "metadata": {
        "id": "OFAWVmqp9l_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = ensemble_model_prediction(data)\n",
        "accuracy_score(y, prediction)"
      ],
      "metadata": {
        "id": "s1tAMvAI9niD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the quality is ideal"
      ],
      "metadata": {
        "id": "fZYPirt69tjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may want to depict regions using function, but we would need to add some code, because the function is designed to work with models that return 0/1 as predictions"
      ],
      "metadata": {
        "id": "NXBeT_dU-pDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "class SimpleEnsemble:\n",
        "  def predict(self, X):\n",
        "    return (1 + ensemble_model_prediction(X)) // 2\n",
        "plot_decision_regions(data, y, SimpleEnsemble())"
      ],
      "metadata": {
        "id": "bmuwMDFE-GHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting classifier"
      ],
      "metadata": {
        "id": "8_2Zxsrw_yYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In previous task we could multiply predictions, but this is not a common tactic (most likely, it will not work another dataset).\n",
        "\n",
        "What may work? - voting\n",
        "\n",
        "Idea: build a lot of different classifiers (Logistic regression, decision tree, ...) and choose the most popular answer anong them"
      ],
      "metadata": {
        "id": "brxYIGxyALN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's work with Iris again"
      ],
      "metadata": {
        "id": "sBc7ISVrBOd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data['data'], columns = data['feature_names'])\n",
        "y = data['target'].copy()\n",
        "y[y!=1] = 0\n",
        "#X = X[['sepal length (cm)', 'sepal width (cm)']]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)"
      ],
      "metadata": {
        "id": "Nse83MO-BG7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build several classification models (see [sklearn documentation](https://scikit-learn.org/1.5/supervised_learning.html) to find out new classifiaction models)\n",
        "\n",
        "All the model from sklearn has `fit` and `predict` methods, so it is easy to work with new models"
      ],
      "metadata": {
        "id": "F9aUlCF7BbMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "print(accuracy_score(y_test,  knn.predict(X_test)))\n",
        "\n",
        "\n",
        "params = {'n_neighbors' : [1, 3, 5, 10, 20, 25, 30, 50]}\n",
        "\n",
        "knn = GridSearchCV(KNeighborsClassifier(),\n",
        "                   params, cv=3,\n",
        "                   scoring='accuracy'\n",
        "                   )\n",
        "knn.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "WHAhFUKZCSRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn.best_score_"
      ],
      "metadata": {
        "id": "iwQ8EqpGCpxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "models = [\n",
        "    LogisticRegression(),\n",
        "    DecisionTreeClassifier(max_depth=4),\n",
        "    KNeighborsClassifier(n_neighbors=5),\n",
        "\n",
        "]\n",
        "\n",
        "titles = ['Logistic Regression', 'Decision Tree', 'KNN']\n",
        "\n",
        "#fit and test every model\n",
        "for model, title in zip(models, titles):\n",
        "  model.fit(X_train, y_train)\n",
        "  print(accuracy_score(y_test, model.predict(X_test)), title)"
      ],
      "metadata": {
        "id": "uTUhqmX4B0AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the models can be used for voting\n",
        "\n",
        "There are 2 types of voting:\n",
        "\n",
        "\n",
        "*   Hard voting - predict the class with majority of votes (if 3 classifier predicted class 1 and one will predict class 0, the result will be class 1)\n",
        "*   Soft voting - preobaibilities, predicted by classifiears are used (if probabilities for class 1 were [0.8, 0.7, 0.9, 0.1] and for class 0 - [0.2, 0.3, 0.1, 0.9], the probability for class 1 will be 0.675, for class 0 = 0.375)\n",
        "\n"
      ],
      "metadata": {
        "id": "mtR7MfFnD4Gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hard voting:"
      ],
      "metadata": {
        "id": "86tw1AdALajD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hard_voting(predictions):\n",
        "  #predictions - list of lists predictions from every model for the sample\n",
        "\n",
        "  pred = []\n",
        "  for prediction_sample in predictions:\n",
        "    #for every class, we know how many models predicted it\n",
        "    classes, votes = np.unique(prediction_sample, return_counts=True)\n",
        "\n",
        "    pred.append(classes[votes.argmax()])\n",
        "  return np.array(pred)"
      ],
      "metadata": {
        "id": "ClPSdjWKFaWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:"
      ],
      "metadata": {
        "id": "by1ouW56GMZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hard_voting([[1, 1, 2, 1, 2, 3],\n",
        "             [2, 2, 2, 2, 2, 2],\n",
        "             [2, 1, 1, 1, 1, 3]])"
      ],
      "metadata": {
        "id": "7srofrzZGJgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply to our data"
      ],
      "metadata": {
        "id": "wWmCNb3eHCRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = np.array([model.predict(X_test) for model in models]).T\n",
        "\n",
        "predictions.shape"
      ],
      "metadata": {
        "id": "CtS3GIbFGytF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_ensemble = hard_voting(predictions)"
      ],
      "metadata": {
        "id": "LS1oSmnjHFsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, predictions_ensemble)"
      ],
      "metadata": {
        "id": "nDI7GpniH_8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soft voting:"
      ],
      "metadata": {
        "id": "cigRYvOlLc0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:"
      ],
      "metadata": {
        "id": "1-b041KChJ_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.iloc[0:1]"
      ],
      "metadata": {
        "id": "g6fT5EwChLom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models[0].predict_proba(X_test.iloc[0:1])"
      ],
      "metadata": {
        "id": "Gxu2c-8EhNGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models[1].predict_proba(X_test.iloc[0:1])"
      ],
      "metadata": {
        "id": "U4oOejcxhNZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models[2].predict_proba(X_test.iloc[0:1])"
      ],
      "metadata": {
        "id": "XkzzeG3vhOGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, lets compute mean probability of class 0:"
      ],
      "metadata": {
        "id": "hmKTv7WehPai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(0.72298819 + 1. + 1.) / 3"
      ],
      "metadata": {
        "id": "d2LXb6GzhPCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And for class 1:\n"
      ],
      "metadata": {
        "id": "_YzxFl5bhWEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(0.27701181 + 0. + 0.) / 3"
      ],
      "metadata": {
        "id": "2d533lAohXfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the mean probability of class 0 if $~0.9$ and for class 1 is $~0.1$, the sample will be classified as class 0."
      ],
      "metadata": {
        "id": "cb2HWjI5hbnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_voting(samples, models):\n",
        "  pred = []\n",
        "  #Let's investigate every sample\n",
        "  for i in range(samples.shape[0]):\n",
        "    proba_sample = 0\n",
        "    #Use every model for the sample\n",
        "    for model in models:\n",
        "      proba_model = model.predict_proba(samples.iloc[i:i+1])\n",
        "      proba_sample += proba_model\n",
        "    #Get new probabilities\n",
        "    proba_sample /= proba_sample.sum()\n",
        "    #Choose class\n",
        "    pred.append(np.argmax(proba_sample))\n",
        "  return np.array(pred)"
      ],
      "metadata": {
        "id": "kwPk_9gWLngC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soft_voting(X_test.iloc[0:1], models)"
      ],
      "metadata": {
        "id": "a9qbaLY4N8A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply to our data"
      ],
      "metadata": {
        "id": "eRDxy9UmM8_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_ensemble = soft_voting(X_test, models)\n",
        "predictions_ensemble"
      ],
      "metadata": {
        "id": "wgfAVtDQO615"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, predictions_ensemble)"
      ],
      "metadata": {
        "id": "T_BzoOLkPDYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented the voting system ourselves, but we also can use the `VotingClassifier` from sklearn:"
      ],
      "metadata": {
        "id": "FImn7QFVerQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "models_names = [\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Decision Tree', DecisionTreeClassifier(max_depth=4)),\n",
        "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
        "\n",
        "]\n",
        "\n",
        "#hard voting\n",
        "voting = VotingClassifier(models_names,\n",
        "                          voting='hard'\n",
        "                          )\n",
        "\n",
        "voting.fit(X_train, y_train)\n",
        "\n",
        "accuracy_score(y_test, voting.predict(X_test))"
      ],
      "metadata": {
        "id": "nubcCFmTe4r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#soft voting\n",
        "voting = VotingClassifier(models_names,\n",
        "                          voting='soft'\n",
        "                          )\n",
        "\n",
        "voting.fit(X_train, y_train)\n",
        "\n",
        "accuracy_score(y_test, voting.predict(X_test))"
      ],
      "metadata": {
        "id": "-tglU-ayfh6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagging"
      ],
      "metadata": {
        "id": "0BcpC_lZQhqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea: generate new samples from our using bootstrap (samples are close enough, so the models wil extract similar real dependencies, but different enough to overfitted models to differ)"
      ],
      "metadata": {
        "id": "-rAqJcyiQkm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's work with some bigger dataset for regression"
      ],
      "metadata": {
        "id": "WgeI5flTQ8Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "california = fetch_california_housing()\n",
        "california_X = pd.DataFrame(data=california.data, columns=california.feature_names)\n",
        "california_Y = california.target\n",
        "print(f\"X shape: {california_X.shape}, Y shape: {california_Y.shape}\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    california_X, california_Y, test_size=0.3, random_state=123, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "8RJmx57rQ7bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's fit a decision tree"
      ],
      "metadata": {
        "id": "8N0IoVKzRLep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree = DecisionTreeRegressor()\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(f\"MSE on train set: {mean_squared_error(y_train, tree.predict(X_train)):.2f}\")\n",
        "print(f\"MSE on test set: {mean_squared_error(y_test, tree.predict(X_test)):.2f}\")"
      ],
      "metadata": {
        "id": "I-pd6Ij9RK_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE on train data is 0.0, but on test data is larger. That seems like overfitting. (Decision tree without constraints can reach ideal quality on non-contradicting data."
      ],
      "metadata": {
        "id": "IiHD-H1v87BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to use bagging. First of all, let's implement bootstrap."
      ],
      "metadata": {
        "id": "aDezR_Ys9zpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bootstrap"
      ],
      "metadata": {
        "id": "vXbHhdBU_kiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea: from our data choose random elements and get new sample with the same number of samples"
      ],
      "metadata": {
        "id": "V3GuNcvB_qEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = np.array([1,2,3,4,5])"
      ],
      "metadata": {
        "id": "Kx5AToDe9YX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing random indices. We need to choose from 0, 1, ..., ‘len(example)-1‘, we allow method to choose same element seceral times, so 'replace=True'."
      ],
      "metadata": {
        "id": "1a4nYNuv_3MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate indices of elems\n",
        "idx = np.random.choice(len(example), replace=True, size=len(example))\n",
        "idx"
      ],
      "metadata": {
        "id": "zWZPIKZ2_Gv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's choose the elements of the initial array"
      ],
      "metadata": {
        "id": "NECi4Dw_iYee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example[idx]"
      ],
      "metadata": {
        "id": "y7Z3sIe_iYFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap(samples):\n",
        "  idx = np.random.choice(len(samples), replace=True, size=len(samples))\n",
        "  return samples[idx].copy()"
      ],
      "metadata": {
        "id": "vlIW0_54i8Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bootstrap(np.array([1,2,3,4,5]))"
      ],
      "metadata": {
        "id": "jvazvbJkjOX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bootstrap(samples, N):\n",
        "  bootstrap_samples = []\n",
        "  for i in range(N):\n",
        "    bootstrap_samples.append(bootstrap(samples))\n",
        "  return np.array(bootstrap_samples)"
      ],
      "metadata": {
        "id": "dKng3cI2lOZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_bootstrap(np.array([1,2,3,4,5]), 3)"
      ],
      "metadata": {
        "id": "TADdO07QmmSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate several bootstrap samples and investigate features."
      ],
      "metadata": {
        "id": "Z0HOKWKLlh_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 8\n",
        "\n",
        "#We need to merge X and y, because the same bootstrap should be applied to both X and y\n",
        "X_y_train = X_train.copy()\n",
        "X_y_train['y'] = y_train.copy()\n",
        "\n",
        "bootstrap_samples = generate_bootstrap(X_y_train.values, N)\n",
        "bootstrap_samples.shape"
      ],
      "metadata": {
        "id": "NiuF0UbIlgd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(ncols=N//2, nrows=2, figsize=(20,10), sharex=True, sharey=True)\n",
        "\n",
        "feature = 2\n",
        "for i in range(N):\n",
        "  axs[i//4][i%4].scatter(bootstrap_samples[i][:,feature], bootstrap_samples[i][:,-1])\n",
        "\n",
        "fig.suptitle('Feature ' + X_y_train.columns[feature] + ' and target', fontsize=16)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "FyqNOk3xnNYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use bagging"
      ],
      "metadata": {
        "id": "-BjvjKv16XSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's fit separate decision tree with every sample we have"
      ],
      "metadata": {
        "id": "NieFnYizoyUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "\n",
        "for sample in bootstrap_samples:\n",
        "  tree = DecisionTreeRegressor()\n",
        "  tree.fit(sample[:,:-1], sample[:,-1])\n",
        "  models.append(tree)"
      ],
      "metadata": {
        "id": "OJ2LVDFko_mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does every tree perform?"
      ],
      "metadata": {
        "id": "guigXENhpbzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tree in models:\n",
        "  print(f\"MSE on test set: {mean_squared_error(y_test, tree.predict(X_test.values)):.2f}\")"
      ],
      "metadata": {
        "id": "bq7jMRQJpRIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's take mean prediction for every sample"
      ],
      "metadata": {
        "id": "jwbomwEopeOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_trees = []\n",
        "for tree in models:\n",
        "  preds_trees.append(tree.predict(X_test.values))\n",
        "preds_trees = np.array(preds_trees)\n",
        "\n",
        "preds_trees.shape"
      ],
      "metadata": {
        "id": "mfRsS4jvpix_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"MSE on test set: {mean_squared_error(y_test, np.mean(preds_trees, axis=0)):.2f}\")"
      ],
      "metadata": {
        "id": "gfPq6VUopwnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MSE on test  decreased! The bagging seems to be working"
      ],
      "metadata": {
        "id": "N86hcuJ5p_3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use the `BaggingRegressor` from sklearn. It will perform bootstrap, fit several models and take mean as prediction"
      ],
      "metadata": {
        "id": "FV1ZBG_lqNs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "base_tree = DecisionTreeRegressor()\n",
        "\n",
        "#we will have 8 trees\n",
        "bag = BaggingRegressor(base_tree,\n",
        "                       n_estimators=8\n",
        "                       )\n",
        "bag.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(f\"MSE on train set: {mean_squared_error(y_train, bag.predict(X_train)):.2f}\")\n",
        "print(f\"MSE on train set: {mean_squared_error(y_test, bag.predict(X_test)):.2f}\")"
      ],
      "metadata": {
        "id": "OmVgqopiqZsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE on train set increases, because not every tree is fitted to predict every sample, so particular overfitted trees may be wrong on train data. MSE on test set is better."
      ],
      "metadata": {
        "id": "6y2jdm97qiQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How many models should we take?"
      ],
      "metadata": {
        "id": "fNz3MYUFqx04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's fit bagging several times with different number of models"
      ],
      "metadata": {
        "id": "YQug9Wvsq9z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_trees = range(1, 100, 4)\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for i in n_trees:\n",
        "    print(i)\n",
        "    bagging = BaggingRegressor(base_tree,\n",
        "                               n_estimators=i,\n",
        "                               n_jobs=4 # this will speed up training a bit\n",
        "\n",
        "                       )\n",
        "    bagging.fit(X_train, y_train)\n",
        "    train_loss.append(mean_squared_error(y_train, bagging.predict(X_train)))\n",
        "    test_loss.append(mean_squared_error(y_test, bagging.predict(X_test)))\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Dependency of MSE and number of models for Bagging\")\n",
        "plt.grid()\n",
        "plt.plot(n_trees, train_loss, label=\"MSE_train\")\n",
        "plt.plot(n_trees, test_loss, label=\"MSE_test\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.xlabel(\"Number of models\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "sjSYYFq4q9Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min(test_loss)"
      ],
      "metadata": {
        "id": "F5xJuakTuiVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we had only 1 tree - MSE was big, we started adding trees and the error begin decreasing. However, after having ~20 trees it stopped decreasing and became almost constant.\n",
        "\n",
        "Why? in the beginning every new tree brings useful information, new predictions, but after we added a lot of trees, next one will not be useful, it will be repeating the answers of previous one and the impact will be smaller."
      ],
      "metadata": {
        "id": "9WIbGuyLsnkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "PjqdehGZrwS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging can be used with any base model (linear regression, decision tree).\n",
        "\n",
        "Using trees is the most popular variant (they can learn non-linear dependencies, but easily overfit). It is possible to apply additional modification to make trees mode diverse - limit the number of features that can be used while spliting nodes in trees. Trees will still be able to discover complex dependencies, but they will significantly differ from each"
      ],
      "metadata": {
        "id": "1eAbkvPwrx9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=8, n_jobs=4)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"MSE on train set: {mean_squared_error(y_train, rf.predict(X_train)):.2f}\")\n",
        "print(f\"MSE on train set: {mean_squared_error(y_test, rf.predict(X_test)):.2f}\")"
      ],
      "metadata": {
        "id": "ieWqhrbQtTug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are similar to the ones of bagging. Let's investigate number of trees:"
      ],
      "metadata": {
        "id": "A7MhDvSWtfiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_trees = range(1, 100, 4)\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for i in n_trees:\n",
        "    print(i)\n",
        "    rf = RandomForestRegressor(n_estimators=i, n_jobs=4)\n",
        "    rf.fit(X_train, y_train)\n",
        "    train_loss.append(mean_squared_error(y_train, rf.predict(X_train)))\n",
        "    test_loss.append(mean_squared_error(y_test, rf.predict(X_test)))\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Dependency of MSE and number of trees for Random Forest\")\n",
        "plt.grid()\n",
        "plt.plot(n_trees, train_loss, label=\"MSE_train\")\n",
        "plt.plot(n_trees, test_loss, label=\"MSE_test\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "w9PB-o1atjPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min(test_loss)"
      ],
      "metadata": {
        "id": "MWZ3f8vjuft8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting"
      ],
      "metadata": {
        "id": "TjgtM7rJufaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea: we fitted one model and it made sone mistekes, let's teach the next model to fix them."
      ],
      "metadata": {
        "id": "T474E6ngu0N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: the item costed 15 rub., model 1 predicted that the item will cost 25rub.. So the  model 2 should try to predict 15rub. - 25rub. = -10rub..\n",
        "\n",
        "Predictions of model 2 cannot be used without predictions of model 1 (item cannot cost -10rub.), so, we will need to sum the predictions to get the final prediction: 25rub. + (-10rub.) = 15rub.\n",
        "\n",
        "\n",
        "$$a(x) = \\sum_{n=1}^N b_n(x)$$\n",
        "\n",
        "$a(x)$ - the prediction of the ensemble\n",
        "\n",
        "$b_n(x)$ - the prediction of the $n$-th base model\n",
        "\n",
        "Model 1 and 2 (base models) are usually trees, but significantly underfitted (not deep, not many leaves, a lot of samples in each leaf). That prevents the overfitting of boosting (if an overfitted model will be fixing the mistakes of overfitted model, the ensemble will be significantly overfitted)"
      ],
      "metadata": {
        "id": "P2Edd7nmvE8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First boosting"
      ],
      "metadata": {
        "id": "A-IUK2Ta1GY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's investigate this idea using synthetic data:"
      ],
      "metadata": {
        "id": "iNtV584Gv_Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generation of samples\n",
        "np.random.seed(123)\n",
        "N = 100\n",
        "X = np.linspace(0, 1, N).reshape(-1, 1)\n",
        "y = np.sin(X)[:, 0] + np.random.normal(0, 0.1, size=N)\n",
        "\n",
        "\n",
        "#Function for visualization\n",
        "def plot_sample_model(\n",
        "    X, y, plot_predictions=False, y_pred=None, y_pred_label=None\n",
        "):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.scatter(X, y, label=\"Train\", alpha=0.7)\n",
        "    if plot_predictions:\n",
        "        plt.plot(X, y_pred, label=y_pred_label, c=\"r\")\n",
        "        plt.title(\"MSE: \" + str(mean_squared_error(y, y_pred)))\n",
        "\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "plot_sample_model(X, y, plot_predictions=False)"
      ],
      "metadata": {
        "id": "B4UbE_xHwDIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 0.* At the beginnig, we have no base models, the ensemble is empty."
      ],
      "metadata": {
        "id": "dlFPYb3aw1Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The prediction of the ensemble\n",
        "a = 0"
      ],
      "metadata": {
        "id": "twNmy5CFxQH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 1* Let's fit first model $b_1(x)$. Let's use decision trees of depth 1 (decision stump)"
      ],
      "metadata": {
        "id": "5_86sO5dxTg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "\n",
        "b = DecisionTreeRegressor(max_depth=1).fit(X, y)\n",
        "\n",
        "\n",
        "a = b.predict(X)\n",
        "\n",
        "\n",
        "plot_sample_model(\n",
        "    X, y, plot_predictions=True, y_pred=a, y_pred_label=\"a = b_1\"\n",
        ")"
      ],
      "metadata": {
        "id": "nXrCXOUQxhvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 2* Let's comute the mistakes (residuals) of the ensemble"
      ],
      "metadata": {
        "id": "Heh6uYkWxpyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = y - a"
      ],
      "metadata": {
        "id": "I--aGQKDyFbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 3* Fit next model ($b_2(x)$). This model tries to predict the residuals. The ensemble will be $a(x) = b_1(x) + b_2(x)$"
      ],
      "metadata": {
        "id": "xxXAdJvIyJUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = DecisionTreeRegressor(max_depth=1).fit(X, s)\n",
        "\n",
        "#Add the prediction of the model to the ensemble\n",
        "a += b.predict(X)\n",
        "\n",
        "plot_sample_model(\n",
        "    X, y, plot_predictions=True, y_pred=a, y_pred_label=\"a = b_1 + b_2\"\n",
        ")"
      ],
      "metadata": {
        "id": "-EHEVjKFyZEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MSE is better and the model is better"
      ],
      "metadata": {
        "id": "hkzeIZoVyqyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 4 - ...* Repeat Steps 2,3 (compute residuals, fit next model, add the new predictions)"
      ],
      "metadata": {
        "id": "0_hRI2U6yuJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = y - a\n",
        "\n",
        "b = DecisionTreeRegressor(max_depth=1).fit(X, s)\n",
        "\n",
        "#Add the prediction of the model to the ensemble\n",
        "a += b.predict(X)\n",
        "\n",
        "plot_sample_model(\n",
        "    X, y, plot_predictions=True, y_pred=a, y_pred_label=\"a = b_1 + b_2 + b_3\"\n",
        ")"
      ],
      "metadata": {
        "id": "3fkRBwoUy6My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More steps - the more complex model, the more precise predictions on the train data.\n",
        "\n",
        "And what will happened on test data? - Most likely, firstly the MSE will decrease (we will be fixing serious mistakes), but after a lot of models we will be overfitting (fixing minor mistakes on train data) and error on test will be increaing.\n",
        "\n",
        "So, the boosting may overfit if we have a lot of models (random forest did not overfit)"
      ],
      "metadata": {
        "id": "U241D1W1y_zP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosting for various models"
      ],
      "metadata": {
        "id": "AxPAubt7zvI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we use only trees? Why not linear regression?"
      ],
      "metadata": {
        "id": "bubQWNxoz2NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This function will help us visualize the process of fitting boosting\n",
        "#(we do not want to copy-paste code again)\n",
        "def plot_boosting_results(b, n_estimators, X, y):\n",
        "    fig, ax = plt.subplots(n_estimators, 3, figsize=(20, n_estimators * 5))\n",
        "\n",
        "    #Residuals\n",
        "    resid = []\n",
        "    resid.append(y)\n",
        "\n",
        "    #Model predictions\n",
        "    y_pred = []\n",
        "\n",
        "    for i in range(n_estimators):\n",
        "        #Training base model\n",
        "        b.fit(X, resid[-1])\n",
        "\n",
        "        #Predicting with base models\n",
        "        y_pred.append(b.predict(X))\n",
        "\n",
        "        #Predicting using ensemble\n",
        "        a = np.sum(y_pred, axis=0)\n",
        "\n",
        "        #Compute residual\n",
        "        resid.append(y - a)\n",
        "\n",
        "        #Prediction of the ensemble (a)\n",
        "        ax[i, 0].scatter(X, y, label=\"Train\", alpha=0.7)\n",
        "        ax[i, 0].plot(X, a, c=\"red\", lw=3, label=\"Number of models = \" + str(i + 1))\n",
        "        ax[i, 0].set_title(\"MSE: \" + str(mean_squared_error(y, a)))\n",
        "\n",
        "        #Prediction of base model (b)\n",
        "        ax[i, 1].scatter(X, resid[-2], label=\"Train\", alpha=0.7)\n",
        "        ax[i, 1].plot(X, y_pred[-1], c=\"red\", lw=3)\n",
        "        ax[i, 1].set_title(\"Prediction of b \" + str(i + 1))\n",
        "\n",
        "        ax[i, 2].scatter(X, resid[-1], alpha=0.7, c=\"orange\")\n",
        "        ax[i, 2].set_title(\"Residuals\")\n",
        "\n",
        "        ax[i, 0].legend()"
      ],
      "metadata": {
        "id": "ps-NqM76z7Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to repeat the first experiment, we may use the function:"
      ],
      "metadata": {
        "id": "dUmY7C6I0yMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_boosting_results(DecisionTreeRegressor(max_depth=1), n_estimators=3, X=X, y=y)"
      ],
      "metadata": {
        "id": "2w5pBjuE0wpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try using linear regression as base model"
      ],
      "metadata": {
        "id": "ZvorcanF1nLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "plot_boosting_results(LinearRegression(), n_estimators=3, X=X, y=y)"
      ],
      "metadata": {
        "id": "XR0Ezznn1rw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strange... We did not get better model by adding second, third base models. They seem to be constant.\n",
        "\n",
        "\n",
        "Why? - Because boosting sums the base models: sum of trees seem to be something complex (hard to say, how the trees may merge), but some of linear models is still a linear model (just different coefficients).\n",
        "\n",
        "The process of building a linear model garantees that we get the best possible (in terms of MSE) first base. The mistakes by first base model cannot be fixed by another linear model.\n",
        "\n",
        "\n",
        "All in all, boosting with trees is the most weel-known."
      ],
      "metadata": {
        "id": "NQaBwKh01usv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosting from sklearn"
      ],
      "metadata": {
        "id": "EfrfsvuW3Ptx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost discussed during lecture:"
      ],
      "metadata": {
        "id": "2AMlN88J300X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "\n",
        "ada = AdaBoostRegressor(n_estimators=3)\n",
        "ada.fit(X, y)\n",
        "\n",
        "plot_sample_model(\n",
        "    X, y, plot_predictions=True, y_pred=ada.predict(X), y_pred_label=\"AdaBoost\"\n",
        ")"
      ],
      "metadata": {
        "id": "Pb6bZHc43S5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient boosting (not discussed, more complex approach, well-known)"
      ],
      "metadata": {
        "id": "TplihWWx33iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "\n",
        "gb = GradientBoostingRegressor(n_estimators=3)\n",
        "gb.fit(X, y)\n",
        "\n",
        "plot_sample_model(\n",
        "    X, y, plot_predictions=True, y_pred=gb.predict(X), y_pred_label=\"Gradient boosting\"\n",
        ")"
      ],
      "metadata": {
        "id": "HoYc8Uce30Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems to give strange bad results... This approach is powerful, but it requires a lot of base models and hyperparameter tuning"
      ],
      "metadata": {
        "id": "KKUJppKx4O0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GradientBoostingRegressor(n_estimators=20)\n",
        "gb.fit(X, y)\n",
        "\n",
        "plot_sample_model(\n",
        "    X, y, plot_predictions=True, y_pred=gb.predict(X), y_pred_label=\"Gradient boosting\"\n",
        ")"
      ],
      "metadata": {
        "id": "LLGnk88e4Fok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfitting"
      ],
      "metadata": {
        "id": "X_4AzAGs4ixT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "X = np.linspace(0, 1, 300).reshape(-1, 1)\n",
        "\n",
        "y = (X > 0.5) + np.random.normal(size=X.shape) * 0.1\n",
        "y = y[:, 0]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(X_train, y_train, label=\"Train\")\n",
        "plt.scatter(X_test, y_test, label=\"Test\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "TQyGtTXa4nQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trees = [1, 2, 5, 20, 100, 500, 1000]\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(len(trees), 2, figsize=(30, 40))\n",
        "\n",
        "loss_rf_train = []\n",
        "loss_gb_train = []\n",
        "loss_rf_test = []\n",
        "loss_gb_test = []\n",
        "\n",
        "for i, ts in enumerate(trees):\n",
        "    rf = RandomForestRegressor(n_estimators=ts, max_depth=3, random_state=123)\n",
        "\n",
        "    #May check GradientBoostingRegressor(max_depth=3,\n",
        "    gb = AdaBoostRegressor(\n",
        "        DecisionTreeRegressor(max_depth=3),\n",
        "        n_estimators=ts,\n",
        "        learning_rate=0.1,\n",
        "        random_state=123,\n",
        "    )\n",
        "\n",
        "    rf.fit(X_train, y_train)\n",
        "    ax[i, 0].scatter(X_train, y_train, label=\"Train\")\n",
        "    ax[i, 0].scatter(X_test, y_test, label=\"Test\")\n",
        "    ax[i, 0].plot(\n",
        "        sorted(X_test),\n",
        "        rf.predict(sorted(X_test)),\n",
        "        lw=3,\n",
        "        c=\"red\",\n",
        "        label=\"Prediction on test data\",\n",
        "    )\n",
        "    ax[i, 0].set_xlabel(\"X\")\n",
        "    ax[i, 0].set_ylabel(\"Y\")\n",
        "    ax[i, 0].set_title(\n",
        "        \"Random Forest, n trees = \"\n",
        "        + str(ts)\n",
        "        + \", MSE = \"\n",
        "        + str(mean_squared_error(y_test, rf.predict(X_test)))\n",
        "    )\n",
        "    ax[i, 0].legend()\n",
        "\n",
        "    loss_rf_train.append(mean_squared_error(y_train, rf.predict(X_train)))\n",
        "    loss_rf_test.append(mean_squared_error(y_test, rf.predict(X_test)))\n",
        "\n",
        "    gb.fit(X_train, y_train)\n",
        "    ax[i, 1].scatter(X_train, y_train, label=\"Train\")\n",
        "    ax[i, 1].scatter(X_test, y_test, label=\"Test\")\n",
        "    ax[i, 1].plot(\n",
        "        sorted(X_test),\n",
        "        gb.predict(sorted(X_test)),\n",
        "        lw=3,\n",
        "        c=\"red\",\n",
        "        label=\"Prediction on test data\",\n",
        "    )\n",
        "    ax[i, 1].set_xlabel(\"X\")\n",
        "    ax[i, 1].set_ylabel(\"Y\")\n",
        "    ax[i, 1].set_title(\n",
        "        \"Boosting, n trees = \"\n",
        "        + str(ts)\n",
        "        + \", MSE = \"\n",
        "        + str(mean_squared_error(y_test, gb.predict(X_test)))\n",
        "    )\n",
        "    ax[i, 1].legend()\n",
        "\n",
        "    loss_gb_train.append(mean_squared_error(y_train, gb.predict(X_train)))\n",
        "    loss_gb_test.append(mean_squared_error(y_test, gb.predict(X_test)))"
      ],
      "metadata": {
        "id": "TRW9Q7Lx4yj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost does not overfit, Gradient boosting may overfit."
      ],
      "metadata": {
        "id": "Qpp7Miv-57PG"
      }
    }
  ]
}